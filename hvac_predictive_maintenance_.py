# -*- coding: utf-8 -*-
"""HVAC predictive maintenance .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12RAPoSlCbyUXpj6EJIW4i9lL8YuMWSbz
"""

# --- Part 1: Load the HVAC Sensor Data ---

import pandas as pd

print("Attempting to load the sensor data file...")

# This dataset is a text file, so we use read_csv, but specify that spaces separate the values.
# We also tell pandas that there is no header row.
try:
    raw_df = pd.read_csv('train_FD001.txt', sep=' ', header=None)
    print("‚úÖ File loaded successfully!")
except FileNotFoundError:
    print("‚ùå Error: 'train_FD001.txt' not found. Please make sure you have uploaded the file to Colab.")
    # Stop the script if the file isn't there
    exit()


# --- Clean up the loaded data ---
# The original file has some extra empty columns at the end, let's drop them.
raw_df.drop(columns=[26, 27], inplace=True)

# --- Assign column names ---
# The dataset description tells us what each column means. Let's give them clear names.
# We'll pretend this is data from an HVAC unit.
column_names = [
    'unit_number', 'time_in_cycles', 'setting_1', 'setting_2', 'setting_3',
    'sensor_1_temp', 'sensor_2_pressure', 'sensor_3_flow', 'sensor_4_vibration',
    'sensor_5', 'sensor_6_rpm', 'sensor_7_pressure', 'sensor_8_temp', 'sensor_9_flow',
    'sensor_10', 'sensor_11_rpm', 'sensor_12_vibration', 'sensor_13_temp', 'sensor_14_flow',
    'sensor_15', 'sensor_16_pressure', 'sensor_17_temp', 'sensor_18', 'sensor_19',
    'sensor_20_rpm', 'sensor_21_vibration'
]
raw_df.columns = column_names

print("\nData has been loaded and column names have been assigned.")

# --- Let's look at our data ---
print("\nHere's a peek at the first 5 rows of our new table:")
# We use .head() again to see the top of our table
print(raw_df.head())

# --- Part 2: Define "Failure" and Create the Target Label ---

import pandas as pd

# This line assumes you have already run the code from Part 1
# and have the 'raw_df' DataFrame in memory.

# --- Step 1: Calculate Remaining Useful Life (RUL) ---
# First, find the last cycle (i.e., the point of failure) for each unit.
max_cycles = raw_df.groupby('unit_number')['time_in_cycles'].max().reset_index()
max_cycles.rename(columns={'time_in_cycles': 'total_lifetime'}, inplace=True)

# Now, merge this total lifetime back into our main table.
# This gives us the failure point for each unit on every row.
df = pd.merge(raw_df, max_cycles, on='unit_number')

# Calculate RUL by subtracting the current time_in_cycles from the total_lifetime.
df['RUL'] = df['total_lifetime'] - df['time_in_cycles']
print("Remaining Useful Life (RUL) calculated for each data point.")


# --- Step 2: Create the Binary Classification Label ---
# We need to decide on a "warning" period before failure.
# Any data point within this period is considered "about to fail".
warning_period = 30 # cycles

# Create the label: 1 if RUL is less than the warning period, 0 otherwise.
df['will_fail_soon'] = (df['RUL'] <= warning_period).astype(int)
print(f"Target label 'will_fail_soon' created with a warning period of {warning_period} cycles.")


# --- Let's look at our data for one unit ---
# We can see the RUL counting down and the 'will_fail_soon' label switching from 0 to 1.
print("\nHere's how the data looks for the last few cycles of Unit 1:")
print(df[df['unit_number'] == 1].tail(35))

# Let's also check the overall balance of our labels
print("\nDistribution of our new label:")
print(df['will_fail_soon'].value_counts(normalize=True))

# --- Part 3: Train the Classification Model ---

from sklearn.model_selection import train_test_split
import lightgbm as lgb
import pandas as pd

# This line assumes you have already run the code from Part 1 & 2
# and have the 'df' DataFrame in memory.

# --- Step 1: Select the Features and Target ---
print("Preparing the 'study materials' for our model...")

# These are the 'clues' our model will use to make predictions.
# We are using all the sensor readings.
feature_cols = [
    'sensor_1_temp', 'sensor_2_pressure', 'sensor_3_flow', 'sensor_4_vibration',
    'sensor_5', 'sensor_6_rpm', 'sensor_7_pressure', 'sensor_8_temp', 'sensor_9_flow',
    'sensor_10', 'sensor_11_rpm', 'sensor_12_vibration', 'sensor_13_temp', 'sensor_14_flow',
    'sensor_15', 'sensor_16_pressure', 'sensor_17_temp', 'sensor_18', 'sensor_19',
    'sensor_20_rpm', 'sensor_21_vibration'
]

# This is the 'answer' we want the model to learn to predict.
target_col = 'will_fail_soon'

X = df[feature_cols]
y = df[target_col]

print(f"‚úÖ Features to be used: {len(feature_cols)} sensor readings.")
print(f"‚úÖ Target to be predicted: '{target_col}'")


# --- Step 2: Split Data into Training and Testing Sets ---
print("\nSplitting the data into a 'study' set and an 'exam' set...")

# We'll use 80% for training and 20% for testing.
# `stratify=y` makes sure both sets have a similar mix of '0's and '1's.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print("‚úÖ Data split complete.")
print(f" - Study set size: {len(X_train)} rows")
print(f" - Exam set size: {len(X_test)} rows")


# --- Step 3: Train the LightGBM (LGBM) Classifier ---
print("\nüß† Now, let the learning begin! Training the LGBM model...")

# Initialize the model with some standard settings.
lgbm = lgb.LGBMClassifier(objective='binary', random_state=42)

# Fit the model to our training data.
lgbm.fit(X_train, y_train)

print("\nüéâ Model training complete! Our model is now ready to make predictions.")

# --- Part 4: Evaluate the Model's Performance ---

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# This line assumes you have already run the code from Part 3 and have
# the trained 'lgbm' model and the test sets (X_test, y_test) in memory.

# --- Step 1: Make Predictions on the Test Data ---
print("Asking the model to take the 'exam' on data it has never seen before...")
y_pred = lgbm.predict(X_test)
print("‚úÖ Predictions are in!")


# --- Step 2: Calculate and Print Performance Metrics ---
# Calculate the overall accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nOverall Model Accuracy: {accuracy:.4f}")

# Print a detailed classification report
# This shows precision, recall, and f1-score for both classes (0 and 1)
print("\n--- Detailed Classification Report ---")
print(classification_report(y_test, y_pred, target_names=['Stable (0)', 'Will Fail Soon (1)']))


# --- Step 3: Visualize the Confusion Matrix ---
print("\n--- Confusion Matrix ---")
# A confusion matrix shows us the types of correct and incorrect predictions.
cm = confusion_matrix(y_test, y_pred)

# Let's plot it to make it easier to read
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Stable', 'Predicted Failure'],
            yticklabels=['Actual Stable', 'Actual Failure'])
plt.title('Confusion Matrix: Model Performance')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

print("\nüéâ Project Complete! You have successfully trained and evaluated a predictive maintenance model.")